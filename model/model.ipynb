{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deployml.keras import NeuralNetworkBase\n",
    "\n",
    "\n",
    "from model_tools_class import mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the option of the number of columns that is shown when looking at the DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id  log_price property_type        room_type  \\\n",
       "0   6901257   5.010635     Apartment  Entire home/apt   \n",
       "1   6304928   5.129899     Apartment  Entire home/apt   \n",
       "2   7919400   4.976734     Apartment  Entire home/apt   \n",
       "3  13418779   6.620073         House  Entire home/apt   \n",
       "4   3808709   4.744932     Apartment  Entire home/apt   \n",
       "\n",
       "                                           amenities  accommodates  bathrooms  \\\n",
       "0  {\"Wireless Internet\",\"Air conditioning\",Kitche...             3        1.0   \n",
       "1  {\"Wireless Internet\",\"Air conditioning\",Kitche...             7        1.0   \n",
       "2  {TV,\"Cable TV\",\"Wireless Internet\",\"Air condit...             5        1.0   \n",
       "3  {TV,\"Cable TV\",Internet,\"Wireless Internet\",Ki...             4        1.0   \n",
       "4  {TV,Internet,\"Wireless Internet\",\"Air conditio...             2        1.0   \n",
       "\n",
       "   bed_type cancellation_policy  cleaning_fee city  \\\n",
       "0  Real Bed              strict          True  NYC   \n",
       "1  Real Bed              strict          True  NYC   \n",
       "2  Real Bed            moderate          True  NYC   \n",
       "3  Real Bed            flexible          True   SF   \n",
       "4  Real Bed            moderate          True   DC   \n",
       "\n",
       "                                         description first_review  \\\n",
       "0  Beautiful, sunlit brownstone 1-bedroom in the ...   2016-06-18   \n",
       "1  Enjoy travelling during your stay in Manhattan...   2017-08-05   \n",
       "2  The Oasis comes complete with a full backyard ...   2017-04-30   \n",
       "3  This light-filled home-away-from-home is super...          NaN   \n",
       "4  Cool, cozy, and comfortable studio located in ...   2015-05-12   \n",
       "\n",
       "  host_has_profile_pic host_identity_verified host_response_rate  host_since  \\\n",
       "0                    t                      t                NaN  2012-03-26   \n",
       "1                    t                      f               100%  2017-06-19   \n",
       "2                    t                      t               100%  2016-10-25   \n",
       "3                    t                      t                NaN  2015-04-19   \n",
       "4                    t                      t               100%  2015-03-01   \n",
       "\n",
       "  instant_bookable last_review   latitude   longitude  \\\n",
       "0                f  2016-07-18  40.696524  -73.991617   \n",
       "1                t  2017-09-23  40.766115  -73.989040   \n",
       "2                t  2017-09-14  40.808110  -73.943756   \n",
       "3                f         NaN  37.772004 -122.431619   \n",
       "4                t  2017-01-22  38.925627  -77.034596   \n",
       "\n",
       "                                       name     neighbourhood  \\\n",
       "0            Beautiful brownstone 1-bedroom  Brooklyn Heights   \n",
       "1  Superb 3BR Apt Located Near Times Square    Hell's Kitchen   \n",
       "2                          The Garden Oasis            Harlem   \n",
       "3        Beautiful Flat in the Heart of SF!      Lower Haight   \n",
       "4                Great studio in midtown DC  Columbia Heights   \n",
       "\n",
       "   number_of_reviews  review_scores_rating  \\\n",
       "0                  2                 100.0   \n",
       "1                  6                  93.0   \n",
       "2                 10                  92.0   \n",
       "3                  0                   NaN   \n",
       "4                  4                  40.0   \n",
       "\n",
       "                                       thumbnail_url  zipcode  bedrooms  beds  \n",
       "0  https://a0.muscache.com/im/pictures/6d7cbbf7-c...    11201       1.0   1.0  \n",
       "1  https://a0.muscache.com/im/pictures/348a55fe-4...    10019       3.0   3.0  \n",
       "2  https://a0.muscache.com/im/pictures/6fae5362-9...    10027       1.0   3.0  \n",
       "3  https://a0.muscache.com/im/pictures/72208dad-9...  94117.0       2.0   2.0  \n",
       "4                                                NaN    20009       0.0   1.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>log_price</th>\n      <th>property_type</th>\n      <th>room_type</th>\n      <th>amenities</th>\n      <th>accommodates</th>\n      <th>bathrooms</th>\n      <th>bed_type</th>\n      <th>cancellation_policy</th>\n      <th>cleaning_fee</th>\n      <th>city</th>\n      <th>description</th>\n      <th>first_review</th>\n      <th>host_has_profile_pic</th>\n      <th>host_identity_verified</th>\n      <th>host_response_rate</th>\n      <th>host_since</th>\n      <th>instant_bookable</th>\n      <th>last_review</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>name</th>\n      <th>neighbourhood</th>\n      <th>number_of_reviews</th>\n      <th>review_scores_rating</th>\n      <th>thumbnail_url</th>\n      <th>zipcode</th>\n      <th>bedrooms</th>\n      <th>beds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6901257</td>\n      <td>5.010635</td>\n      <td>Apartment</td>\n      <td>Entire home/apt</td>\n      <td>{\"Wireless Internet\",\"Air conditioning\",Kitche...</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>Real Bed</td>\n      <td>strict</td>\n      <td>True</td>\n      <td>NYC</td>\n      <td>Beautiful, sunlit brownstone 1-bedroom in the ...</td>\n      <td>2016-06-18</td>\n      <td>t</td>\n      <td>t</td>\n      <td>NaN</td>\n      <td>2012-03-26</td>\n      <td>f</td>\n      <td>2016-07-18</td>\n      <td>40.696524</td>\n      <td>-73.991617</td>\n      <td>Beautiful brownstone 1-bedroom</td>\n      <td>Brooklyn Heights</td>\n      <td>2</td>\n      <td>100.0</td>\n      <td>https://a0.muscache.com/im/pictures/6d7cbbf7-c...</td>\n      <td>11201</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6304928</td>\n      <td>5.129899</td>\n      <td>Apartment</td>\n      <td>Entire home/apt</td>\n      <td>{\"Wireless Internet\",\"Air conditioning\",Kitche...</td>\n      <td>7</td>\n      <td>1.0</td>\n      <td>Real Bed</td>\n      <td>strict</td>\n      <td>True</td>\n      <td>NYC</td>\n      <td>Enjoy travelling during your stay in Manhattan...</td>\n      <td>2017-08-05</td>\n      <td>t</td>\n      <td>f</td>\n      <td>100%</td>\n      <td>2017-06-19</td>\n      <td>t</td>\n      <td>2017-09-23</td>\n      <td>40.766115</td>\n      <td>-73.989040</td>\n      <td>Superb 3BR Apt Located Near Times Square</td>\n      <td>Hell's Kitchen</td>\n      <td>6</td>\n      <td>93.0</td>\n      <td>https://a0.muscache.com/im/pictures/348a55fe-4...</td>\n      <td>10019</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7919400</td>\n      <td>4.976734</td>\n      <td>Apartment</td>\n      <td>Entire home/apt</td>\n      <td>{TV,\"Cable TV\",\"Wireless Internet\",\"Air condit...</td>\n      <td>5</td>\n      <td>1.0</td>\n      <td>Real Bed</td>\n      <td>moderate</td>\n      <td>True</td>\n      <td>NYC</td>\n      <td>The Oasis comes complete with a full backyard ...</td>\n      <td>2017-04-30</td>\n      <td>t</td>\n      <td>t</td>\n      <td>100%</td>\n      <td>2016-10-25</td>\n      <td>t</td>\n      <td>2017-09-14</td>\n      <td>40.808110</td>\n      <td>-73.943756</td>\n      <td>The Garden Oasis</td>\n      <td>Harlem</td>\n      <td>10</td>\n      <td>92.0</td>\n      <td>https://a0.muscache.com/im/pictures/6fae5362-9...</td>\n      <td>10027</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13418779</td>\n      <td>6.620073</td>\n      <td>House</td>\n      <td>Entire home/apt</td>\n      <td>{TV,\"Cable TV\",Internet,\"Wireless Internet\",Ki...</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>Real Bed</td>\n      <td>flexible</td>\n      <td>True</td>\n      <td>SF</td>\n      <td>This light-filled home-away-from-home is super...</td>\n      <td>NaN</td>\n      <td>t</td>\n      <td>t</td>\n      <td>NaN</td>\n      <td>2015-04-19</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>37.772004</td>\n      <td>-122.431619</td>\n      <td>Beautiful Flat in the Heart of SF!</td>\n      <td>Lower Haight</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>https://a0.muscache.com/im/pictures/72208dad-9...</td>\n      <td>94117.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3808709</td>\n      <td>4.744932</td>\n      <td>Apartment</td>\n      <td>Entire home/apt</td>\n      <td>{TV,Internet,\"Wireless Internet\",\"Air conditio...</td>\n      <td>2</td>\n      <td>1.0</td>\n      <td>Real Bed</td>\n      <td>moderate</td>\n      <td>True</td>\n      <td>DC</td>\n      <td>Cool, cozy, and comfortable studio located in ...</td>\n      <td>2015-05-12</td>\n      <td>t</td>\n      <td>t</td>\n      <td>100%</td>\n      <td>2015-03-01</td>\n      <td>t</td>\n      <td>2017-01-22</td>\n      <td>38.925627</td>\n      <td>-77.034596</td>\n      <td>Great studio in midtown DC</td>\n      <td>Columbia Heights</td>\n      <td>4</td>\n      <td>40.0</td>\n      <td>NaN</td>\n      <td>20009</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "df = pd.read_csv(r'../data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in and cleaning the data\n",
    "\n",
    "\n",
    "\n",
    "def wrangle(df):\n",
    "    '''\n",
    "    This function wranlges/prepares the data for usage in the model.\n",
    "    '''\n",
    "\n",
    "    # The daily price of the AirBnb listing.\n",
    "    df['price'] = df['log_price'].apply(lambda x : round(np.exp(x)))\n",
    "\n",
    "    # Cleaning the amenities column.\n",
    "    df['amenities'] = df['amenities'].apply(mt.clean)\n",
    "    df['amenities'] = df['amenities'].apply(lambda x: len(x))\n",
    "\n",
    "    # Applying the get_days function to create a new feature = total number of hosted days.\n",
    "    df['host_since_days'] = df['host_since'].apply(mt.get_days)\n",
    "\n",
    "    # Dropping redundant columns.\n",
    "    df.drop(columns=['host_since','log_price','id','latitude','longitude','name','description','thumbnail_url','review_scores_rating','number_of_reviews','host_has_profile_pic','host_response_rate','last_review','first_review'],inplace=True)\n",
    "\n",
    "    # A new column: Is the room isntantly bookable.\n",
    "    df['instant_bookable'].replace(('f','t'),(\"False\",'True'),inplace=True)\n",
    "\n",
    "    # A new column: is the hosts' identity verified.\n",
    "    df['host_identity_verified'].replace(('f','t'),(\"False\",'True'),inplace=True)\n",
    "\n",
    "    # Dropping null values\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = wrangle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "Y = df['price']\n",
    "X = df.drop(columns=['price'])\n",
    "\n",
    "# moving price to the back of df\n",
    "df['target/price'] = df['price']\n",
    "df.drop(columns=['price'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_strings = X.select_dtypes(include='object')\n",
    "X_non_strings = X.select_dtypes(include=['int64','float'])\n",
    "oe =OrdinalEncoder()\n",
    "oe.fit(X_strings)\n",
    "X_train = oe.transform(X_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_whole = np.concatenate((X_train,X_non_strings),axis=1)\n",
    "\n",
    "model_columns = X_strings.columns.append(X_non_strings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_whole,Y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instantiating the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, input_dim = X_train_whole.shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256,))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(LeakyReLU(alpha=0.05))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.005, decay=5e-4), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n",
      "362/362 [==============================] - 7s 17ms/step - loss: 25266.4707 - val_loss: 26945.2715\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 26945.27148, saving model to Weights\\Weights-001--26945.27148.hdf5\n",
      "Epoch 2/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 19762.8945 - val_loss: 28497.2227\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 26945.27148\n",
      "Epoch 3/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 18996.5430 - val_loss: 28560.5527\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 26945.27148\n",
      "Epoch 4/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 18760.1855 - val_loss: 23604.9941\n",
      "\n",
      "Epoch 00004: val_loss improved from 26945.27148 to 23604.99414, saving model to Weights\\Weights-004--23604.99414.hdf5\n",
      "Epoch 5/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 18225.7559 - val_loss: 20051.0195\n",
      "\n",
      "Epoch 00005: val_loss improved from 23604.99414 to 20051.01953, saving model to Weights\\Weights-005--20051.01953.hdf5\n",
      "Epoch 6/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17986.5215 - val_loss: 18900.8086\n",
      "\n",
      "Epoch 00006: val_loss improved from 20051.01953 to 18900.80859, saving model to Weights\\Weights-006--18900.80859.hdf5\n",
      "Epoch 7/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17895.5625 - val_loss: 18823.5059\n",
      "\n",
      "Epoch 00007: val_loss improved from 18900.80859 to 18823.50586, saving model to Weights\\Weights-007--18823.50586.hdf5\n",
      "Epoch 8/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17931.5703 - val_loss: 19803.5352\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 18823.50586\n",
      "Epoch 9/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17560.8105 - val_loss: 17636.9609\n",
      "\n",
      "Epoch 00009: val_loss improved from 18823.50586 to 17636.96094, saving model to Weights\\Weights-009--17636.96094.hdf5\n",
      "Epoch 10/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17441.0898 - val_loss: 19195.7832\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 17636.96094\n",
      "Epoch 11/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17396.2871 - val_loss: 16973.4043\n",
      "\n",
      "Epoch 00011: val_loss improved from 17636.96094 to 16973.40430, saving model to Weights\\Weights-011--16973.40430.hdf5\n",
      "Epoch 12/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17367.6094 - val_loss: 17522.7051\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 16973.40430\n",
      "Epoch 13/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17303.8945 - val_loss: 18144.5879\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 16973.40430\n",
      "Epoch 14/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17045.8242 - val_loss: 16196.1797\n",
      "\n",
      "Epoch 00014: val_loss improved from 16973.40430 to 16196.17969, saving model to Weights\\Weights-014--16196.17969.hdf5\n",
      "Epoch 15/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17086.8066 - val_loss: 16014.9824\n",
      "\n",
      "Epoch 00015: val_loss improved from 16196.17969 to 16014.98242, saving model to Weights\\Weights-015--16014.98242.hdf5\n",
      "Epoch 16/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17022.1387 - val_loss: 16541.5938\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 16014.98242\n",
      "Epoch 17/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17114.3672 - val_loss: 16500.2852\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 16014.98242\n",
      "Epoch 18/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16954.2773 - val_loss: 17517.0332\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 16014.98242\n",
      "Epoch 19/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17042.2305 - val_loss: 15891.1699\n",
      "\n",
      "Epoch 00019: val_loss improved from 16014.98242 to 15891.16992, saving model to Weights\\Weights-019--15891.16992.hdf5\n",
      "Epoch 20/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 17007.6914 - val_loss: 17454.2598\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 15891.16992\n",
      "Epoch 21/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16928.6094 - val_loss: 19020.4805\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 15891.16992\n",
      "Epoch 22/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16746.6895 - val_loss: 15766.6709\n",
      "\n",
      "Epoch 00022: val_loss improved from 15891.16992 to 15766.67090, saving model to Weights\\Weights-022--15766.67090.hdf5\n",
      "Epoch 23/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16827.8457 - val_loss: 15777.8477\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 15766.67090\n",
      "Epoch 24/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16639.0938 - val_loss: 15800.5254\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 15766.67090\n",
      "Epoch 25/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16748.7734 - val_loss: 15843.9014\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 15766.67090\n",
      "Epoch 26/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16682.4844 - val_loss: 16890.9922\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 15766.67090\n",
      "Epoch 27/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16653.8203 - val_loss: 16510.5117\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 15766.67090\n",
      "Epoch 28/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16666.8359 - val_loss: 16300.3369\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 15766.67090\n",
      "Epoch 29/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16503.2051 - val_loss: 16490.2520\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 15766.67090\n",
      "Epoch 30/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16557.0234 - val_loss: 16412.1836\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 15766.67090\n",
      "Epoch 31/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16403.5918 - val_loss: 16849.8574\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 15766.67090\n",
      "Epoch 32/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16365.6455 - val_loss: 15199.5439\n",
      "\n",
      "Epoch 00032: val_loss improved from 15766.67090 to 15199.54395, saving model to Weights\\Weights-032--15199.54395.hdf5\n",
      "Epoch 33/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16237.1592 - val_loss: 16161.3672\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 15199.54395\n",
      "Epoch 34/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16426.9844 - val_loss: 15598.7529\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 15199.54395\n",
      "Epoch 35/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16331.6758 - val_loss: 15263.5713\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 15199.54395\n",
      "Epoch 36/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16278.5273 - val_loss: 16175.7412\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 15199.54395\n",
      "Epoch 37/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16286.6553 - val_loss: 15858.5674\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 15199.54395\n",
      "Epoch 38/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15986.1514 - val_loss: 16099.2852\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 15199.54395\n",
      "Epoch 39/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16107.6924 - val_loss: 15418.9170\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 15199.54395\n",
      "Epoch 40/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16122.6875 - val_loss: 15179.0918\n",
      "\n",
      "Epoch 00040: val_loss improved from 15199.54395 to 15179.09180, saving model to Weights\\Weights-040--15179.09180.hdf5\n",
      "Epoch 41/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15872.6064 - val_loss: 14955.1377\n",
      "\n",
      "Epoch 00041: val_loss improved from 15179.09180 to 14955.13770, saving model to Weights\\Weights-041--14955.13770.hdf5\n",
      "Epoch 42/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16077.8857 - val_loss: 15193.8838\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 14955.13770\n",
      "Epoch 43/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16053.9609 - val_loss: 14832.4199\n",
      "\n",
      "Epoch 00043: val_loss improved from 14955.13770 to 14832.41992, saving model to Weights\\Weights-043--14832.41992.hdf5\n",
      "Epoch 44/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16008.8477 - val_loss: 17177.6328\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 14832.41992\n",
      "Epoch 45/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16092.5078 - val_loss: 15112.1250\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 14832.41992\n",
      "Epoch 46/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 16071.7490 - val_loss: 14648.5254\n",
      "\n",
      "Epoch 00046: val_loss improved from 14832.41992 to 14648.52539, saving model to Weights\\Weights-046--14648.52539.hdf5\n",
      "Epoch 47/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15825.5586 - val_loss: 14747.1074\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 14648.52539\n",
      "Epoch 48/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15795.4023 - val_loss: 14785.2637\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 14648.52539\n",
      "Epoch 49/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15961.9834 - val_loss: 14673.4229\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 14648.52539\n",
      "Epoch 50/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15815.9355 - val_loss: 14700.3291\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 14648.52539\n",
      "Epoch 51/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15966.5420 - val_loss: 14621.3789\n",
      "\n",
      "Epoch 00051: val_loss improved from 14648.52539 to 14621.37891, saving model to Weights\\Weights-051--14621.37891.hdf5\n",
      "Epoch 52/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15867.6807 - val_loss: 14592.7734\n",
      "\n",
      "Epoch 00052: val_loss improved from 14621.37891 to 14592.77344, saving model to Weights\\Weights-052--14592.77344.hdf5\n",
      "Epoch 53/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15713.0000 - val_loss: 14637.6201\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 14592.77344\n",
      "Epoch 54/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15596.0137 - val_loss: 14572.5283\n",
      "\n",
      "Epoch 00054: val_loss improved from 14592.77344 to 14572.52832, saving model to Weights\\Weights-054--14572.52832.hdf5\n",
      "Epoch 55/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15766.8945 - val_loss: 14712.1670\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 14572.52832\n",
      "Epoch 56/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15762.2588 - val_loss: 15566.7520\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 14572.52832\n",
      "Epoch 57/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15646.7090 - val_loss: 15218.9639\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 14572.52832\n",
      "Epoch 58/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15431.4688 - val_loss: 14890.6455\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 14572.52832\n",
      "Epoch 59/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15674.5625 - val_loss: 14625.8086\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 14572.52832\n",
      "Epoch 60/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15507.0850 - val_loss: 15245.9023\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 14572.52832\n",
      "Epoch 61/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15729.3936 - val_loss: 14479.9590\n",
      "\n",
      "Epoch 00061: val_loss improved from 14572.52832 to 14479.95898, saving model to Weights\\Weights-061--14479.95898.hdf5\n",
      "Epoch 62/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15491.0537 - val_loss: 14596.7842\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 14479.95898\n",
      "Epoch 63/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15554.5615 - val_loss: 14579.0283\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 14479.95898\n",
      "Epoch 64/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15473.4766 - val_loss: 15337.8135\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 14479.95898\n",
      "Epoch 65/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15475.7061 - val_loss: 14579.5312\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 14479.95898\n",
      "Epoch 66/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15479.3320 - val_loss: 14419.0654\n",
      "\n",
      "Epoch 00066: val_loss improved from 14479.95898 to 14419.06543, saving model to Weights\\Weights-066--14419.06543.hdf5\n",
      "Epoch 67/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15540.4092 - val_loss: 14534.8477\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 14419.06543\n",
      "Epoch 68/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15469.2422 - val_loss: 14950.4756\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 14419.06543\n",
      "Epoch 69/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15456.5645 - val_loss: 15312.9004\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 14419.06543\n",
      "Epoch 70/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15469.6855 - val_loss: 14699.8984\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 14419.06543\n",
      "Epoch 71/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15297.5928 - val_loss: 14362.2998\n",
      "\n",
      "Epoch 00071: val_loss improved from 14419.06543 to 14362.29980, saving model to Weights\\Weights-071--14362.29980.hdf5\n",
      "Epoch 72/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15409.2168 - val_loss: 14432.9766\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 14362.29980\n",
      "Epoch 73/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15353.7539 - val_loss: 14353.8818\n",
      "\n",
      "Epoch 00073: val_loss improved from 14362.29980 to 14353.88184, saving model to Weights\\Weights-073--14353.88184.hdf5\n",
      "Epoch 74/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15432.7656 - val_loss: 14494.2305\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 14353.88184\n",
      "Epoch 75/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15458.2119 - val_loss: 14592.7588\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 14353.88184\n",
      "Epoch 76/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15533.1357 - val_loss: 14450.3320\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 14353.88184\n",
      "Epoch 77/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15355.1484 - val_loss: 14409.4512\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 14353.88184\n",
      "Epoch 78/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15393.7510 - val_loss: 14988.9707\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 14353.88184\n",
      "Epoch 79/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15345.1855 - val_loss: 14988.8945\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 14353.88184\n",
      "Epoch 80/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15267.6436 - val_loss: 14396.5869\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 14353.88184\n",
      "Epoch 81/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15324.7061 - val_loss: 14552.5449\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 14353.88184\n",
      "Epoch 82/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15269.1289 - val_loss: 14785.5713\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 14353.88184\n",
      "Epoch 83/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15402.2119 - val_loss: 14327.3770\n",
      "\n",
      "Epoch 00083: val_loss improved from 14353.88184 to 14327.37695, saving model to Weights\\Weights-083--14327.37695.hdf5\n",
      "Epoch 84/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15187.3809 - val_loss: 14741.8750\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 14327.37695\n",
      "Epoch 85/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15133.1709 - val_loss: 14298.9121\n",
      "\n",
      "Epoch 00085: val_loss improved from 14327.37695 to 14298.91211, saving model to Weights\\Weights-085--14298.91211.hdf5\n",
      "Epoch 86/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15156.0176 - val_loss: 14848.8330\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 14298.91211\n",
      "Epoch 87/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15099.8779 - val_loss: 14653.7227\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 14298.91211\n",
      "Epoch 88/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15155.5596 - val_loss: 14350.8145\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 14298.91211\n",
      "Epoch 89/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15198.8525 - val_loss: 15452.6523\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 14298.91211\n",
      "Epoch 90/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15194.3682 - val_loss: 14527.6777\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 14298.91211\n",
      "Epoch 91/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15173.2422 - val_loss: 14355.7861\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 14298.91211\n",
      "Epoch 92/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15237.4512 - val_loss: 14374.3076\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 14298.91211\n",
      "Epoch 93/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15179.1953 - val_loss: 14339.5283\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 14298.91211\n",
      "Epoch 94/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15072.6006 - val_loss: 15469.0850\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 14298.91211\n",
      "Epoch 95/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15254.5518 - val_loss: 14279.6299\n",
      "\n",
      "Epoch 00095: val_loss improved from 14298.91211 to 14279.62988, saving model to Weights\\Weights-095--14279.62988.hdf5\n",
      "Epoch 96/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15120.6357 - val_loss: 15122.9473\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 14279.62988\n",
      "Epoch 97/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15156.6934 - val_loss: 15018.4043\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 14279.62988\n",
      "Epoch 98/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15177.8760 - val_loss: 14753.4189\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 14279.62988\n",
      "Epoch 99/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15049.5908 - val_loss: 14407.5508\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 14279.62988\n",
      "Epoch 100/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15059.3418 - val_loss: 15112.2100\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 14279.62988\n",
      "Epoch 101/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15116.5068 - val_loss: 14576.2246\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 14279.62988\n",
      "Epoch 102/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14941.6377 - val_loss: 14940.0537\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 14279.62988\n",
      "Epoch 103/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15123.8203 - val_loss: 14313.3740\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 14279.62988\n",
      "Epoch 104/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15142.9111 - val_loss: 14220.0791\n",
      "\n",
      "Epoch 00104: val_loss improved from 14279.62988 to 14220.07910, saving model to Weights\\Weights-104--14220.07910.hdf5\n",
      "Epoch 105/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14994.9287 - val_loss: 14418.4170\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 14220.07910\n",
      "Epoch 106/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15007.9756 - val_loss: 14259.9902\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 14220.07910\n",
      "Epoch 107/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15076.1768 - val_loss: 14333.1133\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 14220.07910\n",
      "Epoch 108/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14929.2256 - val_loss: 14248.1279\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 14220.07910\n",
      "Epoch 109/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15027.3730 - val_loss: 14303.2822\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 14220.07910\n",
      "Epoch 110/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15050.3857 - val_loss: 14303.9160\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 14220.07910\n",
      "Epoch 111/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14906.2549 - val_loss: 14537.5996\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 14220.07910\n",
      "Epoch 112/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15009.0703 - val_loss: 14279.8506\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 14220.07910\n",
      "Epoch 113/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15133.7305 - val_loss: 14193.6270\n",
      "\n",
      "Epoch 00113: val_loss improved from 14220.07910 to 14193.62695, saving model to Weights\\Weights-113--14193.62695.hdf5\n",
      "Epoch 114/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14956.4033 - val_loss: 14332.4844\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 14193.62695\n",
      "Epoch 115/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14768.5361 - val_loss: 14217.3154\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 14193.62695\n",
      "Epoch 116/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14941.8896 - val_loss: 14196.5020\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 14193.62695\n",
      "Epoch 117/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14998.5742 - val_loss: 14212.9863\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 14193.62695\n",
      "Epoch 118/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14909.4004 - val_loss: 14747.7266\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 14193.62695\n",
      "Epoch 119/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15066.4941 - val_loss: 14600.0439\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 14193.62695\n",
      "Epoch 120/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14955.4834 - val_loss: 15058.7510\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 14193.62695\n",
      "Epoch 121/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14958.3779 - val_loss: 14459.3594\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 14193.62695\n",
      "Epoch 122/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14956.5322 - val_loss: 14248.1104\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 14193.62695\n",
      "Epoch 123/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14900.7451 - val_loss: 14273.0430\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 14193.62695\n",
      "Epoch 124/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14853.9365 - val_loss: 14441.0469\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 14193.62695\n",
      "Epoch 125/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14768.3564 - val_loss: 14240.4043\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 14193.62695\n",
      "Epoch 126/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14893.3242 - val_loss: 14267.3770\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 14193.62695\n",
      "Epoch 127/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14937.9951 - val_loss: 14552.0645\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 14193.62695\n",
      "Epoch 128/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14930.8721 - val_loss: 14714.0127\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 14193.62695\n",
      "Epoch 129/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14872.7666 - val_loss: 14420.0273\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 14193.62695\n",
      "Epoch 130/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14904.3984 - val_loss: 14351.8965\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 14193.62695\n",
      "Epoch 131/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 15025.5752 - val_loss: 14507.4209\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 14193.62695\n",
      "Epoch 132/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14749.9639 - val_loss: 14371.8496\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 14193.62695\n",
      "Epoch 133/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14807.9863 - val_loss: 14647.0127\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 14193.62695\n",
      "Epoch 134/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14864.1885 - val_loss: 14248.0273\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 14193.62695\n",
      "Epoch 135/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14864.9756 - val_loss: 14285.6924\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 14193.62695\n",
      "Epoch 136/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14802.7393 - val_loss: 14398.9668\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 14193.62695\n",
      "Epoch 137/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14970.3486 - val_loss: 14258.1045\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 14193.62695\n",
      "Epoch 138/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14899.8828 - val_loss: 14180.2412\n",
      "\n",
      "Epoch 00138: val_loss improved from 14193.62695 to 14180.24121, saving model to Weights\\Weights-138--14180.24121.hdf5\n",
      "Epoch 139/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14846.1416 - val_loss: 14219.5195\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 14180.24121\n",
      "Epoch 140/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14867.2686 - val_loss: 14422.7178\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 14180.24121\n",
      "Epoch 141/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14752.7217 - val_loss: 14278.7725\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 14180.24121\n",
      "Epoch 142/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14803.2969 - val_loss: 14336.8643\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 14180.24121\n",
      "Epoch 143/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14686.2959 - val_loss: 14243.1045\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 14180.24121\n",
      "Epoch 144/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14818.6387 - val_loss: 14645.9023\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 14180.24121\n",
      "Epoch 145/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14716.5928 - val_loss: 14247.6396\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 14180.24121\n",
      "Epoch 146/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14707.5156 - val_loss: 14276.4277\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 14180.24121\n",
      "Epoch 147/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14915.1768 - val_loss: 14443.8916\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 14180.24121\n",
      "Epoch 148/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14644.1621 - val_loss: 14305.0596\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 14180.24121\n",
      "Epoch 149/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14655.8896 - val_loss: 14234.0693\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 14180.24121\n",
      "Epoch 150/150\n",
      "362/362 [==============================] - 6s 17ms/step - loss: 14827.2939 - val_loss: 14481.5020\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 14180.24121\n"
     ]
    }
   ],
   "source": [
    "checkpoint_name = 'Weights\\Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(x=X_train,y=y_train.values,\n",
    "            validation_data=(X_val,y_val.values),\n",
    "            batch_size=128,epochs=150, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 124
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-26T16:25:10.780898</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 388.0125 248.518125 \r\nL 388.0125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\nL 380.8125 7.2 \r\nL 46.0125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m9e1fa8696d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(58.049432 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.084861\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(95.722361 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"142.939041\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(136.576541 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"183.79322\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(177.43072 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.647399\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(218.284899 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.501579\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(255.957829 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.355758\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(296.812008 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"347.209937\" xlink:href=\"#m9e1fa8696d\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 140 -->\r\n      <g transform=\"translate(337.666187 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m334d53f1bf\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"217.233971\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 14000 -->\r\n      <g transform=\"translate(7.2 221.03319)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"189.741837\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 16000 -->\r\n      <g transform=\"translate(7.2 193.541056)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"162.249703\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 18000 -->\r\n      <g transform=\"translate(7.2 166.048922)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"134.757569\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 20000 -->\r\n      <g transform=\"translate(7.2 138.556788)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"107.265435\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 22000 -->\r\n      <g transform=\"translate(7.2 111.064653)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"79.7733\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 24000 -->\r\n      <g transform=\"translate(7.2 83.572519)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"52.281166\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 26000 -->\r\n      <g transform=\"translate(7.2 56.080385)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#m334d53f1bf\" y=\"24.789032\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 28000 -->\r\n      <g transform=\"translate(7.2 28.588251)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p28ac8c8b06)\" d=\"M 61.230682 62.364309 \r\nL 63.273391 138.016836 \r\nL 65.3161 148.551156 \r\nL 67.358809 151.800141 \r\nL 69.401518 159.146448 \r\nL 71.444227 162.43498 \r\nL 73.486936 163.685308 \r\nL 75.529645 163.190342 \r\nL 77.572354 168.286831 \r\nL 79.615063 169.932519 \r\nL 81.657772 170.548381 \r\nL 83.70048 170.942587 \r\nL 85.743189 171.818415 \r\nL 87.785898 175.365867 \r\nL 89.828607 174.80252 \r\nL 91.871316 175.69145 \r\nL 93.914025 174.423671 \r\nL 95.956734 176.624277 \r\nL 97.999443 175.415267 \r\nL 100.042152 175.890043 \r\nL 102.084861 176.97711 \r\nL 104.12757 179.477794 \r\nL 106.170279 178.362215 \r\nL 108.212988 180.956812 \r\nL 110.255697 179.449147 \r\nL 112.298406 180.360361 \r\nL 114.341115 180.754379 \r\nL 116.383824 180.575466 \r\nL 118.426533 182.824746 \r\nL 120.469242 182.084956 \r\nL 122.511951 184.194037 \r\nL 124.55466 184.71565 \r\nL 126.597369 186.481831 \r\nL 128.640078 183.872481 \r\nL 130.682787 185.1826 \r\nL 132.725496 185.913182 \r\nL 134.768205 185.801455 \r\nL 136.810914 189.932201 \r\nL 138.853623 188.26149 \r\nL 140.896332 188.055367 \r\nL 142.939041 191.492998 \r\nL 144.98175 188.671215 \r\nL 147.024459 189.000087 \r\nL 149.067167 189.620217 \r\nL 151.109876 188.470219 \r\nL 153.152585 188.75557 \r\nL 155.195294 192.13972 \r\nL 157.238003 192.55425 \r\nL 159.280712 190.264416 \r\nL 161.323421 192.272 \r\nL 163.36613 190.201753 \r\nL 165.408839 191.560708 \r\nL 167.451548 193.686958 \r\nL 169.494257 195.29506 \r\nL 171.536966 192.946121 \r\nL 173.579675 193.009844 \r\nL 175.622384 194.598199 \r\nL 177.665093 197.556906 \r\nL 179.707802 194.215323 \r\nL 181.750511 196.51748 \r\nL 183.79322 193.461612 \r\nL 185.835929 196.737847 \r\nL 187.878638 195.864864 \r\nL 189.921347 196.979464 \r\nL 194.006765 196.898974 \r\nL 196.049474 196.059403 \r\nL 198.092183 197.03767 \r\nL 200.134892 197.211939 \r\nL 202.177601 197.031575 \r\nL 204.22031 199.397174 \r\nL 206.263019 197.862783 \r\nL 208.305728 198.625179 \r\nL 210.348437 197.539079 \r\nL 212.391146 197.189293 \r\nL 214.433854 196.159385 \r\nL 216.476563 198.60601 \r\nL 218.519272 198.075377 \r\nL 220.561981 198.742961 \r\nL 222.60469 199.808858 \r\nL 224.647399 199.024473 \r\nL 226.690108 199.78844 \r\nL 228.732817 197.959072 \r\nL 230.775526 200.912154 \r\nL 232.818235 201.657328 \r\nL 234.860944 201.343276 \r\nL 236.903653 202.114976 \r\nL 238.946362 201.349572 \r\nL 240.989071 200.754464 \r\nL 243.03178 200.816106 \r\nL 245.074489 201.106506 \r\nL 247.117198 200.223885 \r\nL 249.159907 201.024674 \r\nL 251.202616 202.489932 \r\nL 253.245325 199.988819 \r\nL 255.288034 201.829637 \r\nL 257.330743 201.333987 \r\nL 259.373452 201.042809 \r\nL 261.416161 202.806226 \r\nL 263.45887 202.672188 \r\nL 265.501579 201.886394 \r\nL 267.544288 204.290156 \r\nL 269.586997 201.785862 \r\nL 271.629706 201.523438 \r\nL 273.672415 203.557615 \r\nL 275.715124 203.378271 \r\nL 277.757833 202.440773 \r\nL 279.800541 204.460774 \r\nL 281.84325 203.111633 \r\nL 283.885959 202.795299 \r\nL 285.928668 204.776531 \r\nL 287.971377 203.363223 \r\nL 290.014086 201.649636 \r\nL 294.099504 206.669622 \r\nL 296.142213 204.286693 \r\nL 298.184922 203.507503 \r\nL 300.227631 204.733293 \r\nL 302.27034 202.573871 \r\nL 304.313049 204.099833 \r\nL 308.398467 204.085415 \r\nL 310.441176 204.852269 \r\nL 312.483885 205.495703 \r\nL 314.526594 206.672092 \r\nL 316.569303 204.954277 \r\nL 318.612012 204.340228 \r\nL 320.654721 204.438141 \r\nL 322.69743 205.236863 \r\nL 324.740139 204.80205 \r\nL 326.782848 203.136346 \r\nL 328.825557 206.924918 \r\nL 332.910975 205.354779 \r\nL 334.953684 205.343959 \r\nL 336.996393 206.199464 \r\nL 339.039102 203.895494 \r\nL 341.081811 204.864122 \r\nL 343.12452 205.602852 \r\nL 345.167228 205.31244 \r\nL 347.209937 206.887009 \r\nL 349.252646 206.191799 \r\nL 351.295355 207.800102 \r\nL 353.338064 205.980909 \r\nL 355.380773 207.383639 \r\nL 357.423482 207.508414 \r\nL 359.466191 204.65389 \r\nL 361.5089 208.379276 \r\nL 363.551609 208.218068 \r\nL 365.594318 205.861933 \r\nL 365.594318 205.861933 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p28ac8c8b06)\" d=\"M 61.230682 39.287401 \r\nL 63.273391 17.954176 \r\nL 65.3161 17.083636 \r\nL 67.358809 85.203077 \r\nL 69.401518 134.056251 \r\nL 71.444227 149.867128 \r\nL 73.486936 150.929736 \r\nL 75.529645 137.458188 \r\nL 77.572354 167.240062 \r\nL 79.615063 145.812387 \r\nL 81.657772 176.361356 \r\nL 83.70048 168.810631 \r\nL 85.743189 160.262188 \r\nL 87.785898 187.045138 \r\nL 89.828607 189.535888 \r\nL 91.871316 182.297053 \r\nL 93.914025 182.864884 \r\nL 95.956734 168.888597 \r\nL 97.999443 191.237823 \r\nL 102.084861 148.22211 \r\nL 104.12757 192.949195 \r\nL 106.170279 192.795558 \r\nL 108.212988 192.483829 \r\nL 110.255697 191.887579 \r\nL 112.298406 177.494199 \r\nL 114.341115 182.724309 \r\nL 116.383824 185.613386 \r\nL 118.426533 183.002801 \r\nL 120.469242 184.075934 \r\nL 122.511951 178.05964 \r\nL 124.55466 200.74496 \r\nL 126.597369 187.523673 \r\nL 128.640078 195.257406 \r\nL 130.682787 199.864836 \r\nL 132.725496 187.326087 \r\nL 134.768205 191.685979 \r\nL 136.810914 188.377057 \r\nL 138.853623 197.729443 \r\nL 140.896332 201.026096 \r\nL 142.939041 204.104585 \r\nL 144.98175 200.822765 \r\nL 147.024459 205.791471 \r\nL 149.067167 173.554018 \r\nL 151.109876 201.946627 \r\nL 153.152585 208.319298 \r\nL 155.195294 206.964183 \r\nL 157.238003 206.439684 \r\nL 159.280712 207.977056 \r\nL 161.323421 207.607201 \r\nL 163.36613 208.692455 \r\nL 165.408839 209.085668 \r\nL 167.451548 208.469202 \r\nL 169.494257 209.363959 \r\nL 171.536966 207.444476 \r\nL 173.579675 195.697294 \r\nL 177.665093 204.991098 \r\nL 179.707802 208.631564 \r\nL 181.750511 200.107714 \r\nL 183.79322 210.636423 \r\nL 185.835929 209.030536 \r\nL 187.878638 209.274609 \r\nL 189.921347 198.844298 \r\nL 191.964056 209.267696 \r\nL 194.006765 211.47347 \r\nL 196.049474 209.88192 \r\nL 198.092183 204.16867 \r\nL 200.134892 199.186755 \r\nL 202.177601 207.613121 \r\nL 204.22031 212.253774 \r\nL 206.263019 211.282247 \r\nL 208.305728 212.369488 \r\nL 210.348437 210.440246 \r\nL 212.391146 209.085869 \r\nL 214.433854 211.043677 \r\nL 216.476563 211.605628 \r\nL 218.519272 203.639514 \r\nL 220.561981 203.640561 \r\nL 222.60469 211.782461 \r\nL 224.647399 209.638652 \r\nL 226.690108 206.435456 \r\nL 228.732817 212.733826 \r\nL 230.775526 207.036108 \r\nL 232.818235 213.125106 \r\nL 234.860944 205.565856 \r\nL 236.903653 208.247856 \r\nL 238.946362 212.411652 \r\nL 240.989071 197.265715 \r\nL 243.03178 209.980478 \r\nL 245.074489 212.343311 \r\nL 247.117198 212.088714 \r\nL 249.159907 212.566792 \r\nL 251.202616 197.039831 \r\nL 253.245325 213.39016 \r\nL 255.288034 201.797863 \r\nL 257.330743 203.234918 \r\nL 259.373452 206.877424 \r\nL 261.416161 211.631751 \r\nL 263.45887 201.945459 \r\nL 265.501579 209.313149 \r\nL 267.544288 204.31193 \r\nL 269.586997 212.926311 \r\nL 271.629706 214.208749 \r\nL 273.672415 211.482383 \r\nL 275.715124 213.660128 \r\nL 277.757833 212.654974 \r\nL 279.800541 213.823188 \r\nL 281.84325 213.065034 \r\nL 283.885959 213.056321 \r\nL 285.928668 209.844091 \r\nL 287.971377 213.387126 \r\nL 290.014086 214.572362 \r\nL 292.056795 212.663619 \r\nL 294.099504 214.246739 \r\nL 296.142213 214.532842 \r\nL 298.184922 214.306247 \r\nL 300.227631 206.955672 \r\nL 302.27034 208.985727 \r\nL 304.313049 202.680309 \r\nL 306.355758 210.919587 \r\nL 308.398467 213.82343 \r\nL 310.441176 213.480704 \r\nL 312.483885 211.171311 \r\nL 314.526594 213.929358 \r\nL 316.569303 213.55859 \r\nL 318.612012 209.645256 \r\nL 320.654721 207.419105 \r\nL 322.69743 211.460247 \r\nL 324.740139 212.396779 \r\nL 326.782848 210.25893 \r\nL 328.825557 212.122502 \r\nL 330.868266 208.340091 \r\nL 332.910975 213.824571 \r\nL 334.953684 213.306825 \r\nL 336.996393 211.749747 \r\nL 339.039102 213.68605 \r\nL 341.081811 214.756364 \r\nL 343.12452 214.216441 \r\nL 345.167228 211.423265 \r\nL 347.209937 213.401946 \r\nL 349.252646 212.603413 \r\nL 351.295355 213.892241 \r\nL 353.338064 208.355354 \r\nL 355.380773 213.8299 \r\nL 357.423482 213.434177 \r\nL 359.466191 211.132208 \r\nL 361.5089 213.040602 \r\nL 363.551609 214.016439 \r\nL 365.594318 210.615213 \r\nL 365.594318 210.615213 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 46.0125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 380.8125 224.64 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 46.0125 224.64 \r\nL 380.8125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 46.0125 7.2 \r\nL 380.8125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 302.671875 44.834375 \r\nL 373.8125 44.834375 \r\nQ 375.8125 44.834375 375.8125 42.834375 \r\nL 375.8125 14.2 \r\nQ 375.8125 12.2 373.8125 12.2 \r\nL 302.671875 12.2 \r\nQ 300.671875 12.2 300.671875 14.2 \r\nL 300.671875 42.834375 \r\nQ 300.671875 44.834375 302.671875 44.834375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 304.671875 20.298437 \r\nL 324.671875 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_17\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(332.671875 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 304.671875 34.976562 \r\nL 324.671875 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\"/>\r\n    <g id=\"text_18\">\r\n     <!-- val_loss -->\r\n     <g transform=\"translate(332.671875 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 191 3500 \r\nL 800 3500 \r\nL 1894 563 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2284 0 \r\nL 1503 0 \r\nL 191 3500 \r\nz\r\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3263 -1063 \r\nL 3263 -1509 \r\nL -63 -1509 \r\nL -63 -1063 \r\nL 3263 -1063 \r\nz\r\n\" id=\"DejaVuSans-5f\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-5f\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p28ac8c8b06\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "57.9709095675533"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "mean_absolute_error(y_val,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: Airbnb_models\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Airbnb_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('my_pickle_model', 'wb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd0df67e98752399f1df19cca3c6bce828e5600fc8fe66d1d5fce4520b14f4d0349",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}